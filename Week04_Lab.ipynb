{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrrnjFM8GY9k"
   },
   "outputs": [],
   "source": [
    "# Setting up google drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FG13oXIzHdE_"
   },
   "outputs": [],
   "source": [
    "import my_utils as mu\n",
    "import torch\n",
    "from torch import nn\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFiPPdSrGY9p"
   },
   "source": [
    "# The Task\n",
    "\n",
    "* Our goal for this week is to ensure that we have understood the pipeline for training a model using PyTorch. The pipeline consists of:\n",
    "    1. Read the dataset and create the appropriate dataloaders. \n",
    "    1. Create and initialise the model.\n",
    "    1. Create the loss and the optimizer.\n",
    "    1. Create the training function.\n",
    "    1. Train and evaluate the model.\n",
    "* Hence there will be no new **Task** for this week but you should have more time to:\n",
    "    1. Go through the task from Week 3 and if needed ask us additional questions.\n",
    "    1. Run the notebook from Week 4 on Softmax Regression. For your convenience the notebook is provided below. Make sure you understand the provided pipeline!\n",
    "    1. If you have done so there are **2 optional tasks** at the end of the notebook!\n",
    "    \n",
    "* The Learning Outcome: Hands-on application of PyTorch's API for solving Softmax Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0vNYfIfGY9q",
    "origin_pos": 0
   },
   "source": [
    "# Concise Implementation of Softmax Regression\n",
    "\n",
    "* Goal: use high-level APIs of PyTorch for implementing Softmax Regression for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2vjE9dMGY9q",
    "origin_pos": 4,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = mu.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0lcxsj-GY9q",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = next(iter(train_iter)) # first batch\n",
    "print(X.size())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb6LJQO1GY9r",
    "origin_pos": 5
   },
   "source": [
    "# Defining the Model and Initialization\n",
    "\n",
    "* Each example is represented by a fixed-length vector: we flatten each $28 \\times 28$ image, treating it as vector of length 784.\n",
    "\n",
    "* Because our dataset has 10 classes, our network will have an output dimension of 10.\n",
    "* So, our weights `W` will be a $784 \\times 10$ matrix and the biases `b` will constitute a $10 \\times 1$ row vector.\n",
    "* We initialize `W` using a Gaussian distribution and `b` with 0.\n",
    "* Softmax regression can be implemented as a Fully-Connected (i.e Linear) layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqPybKEWGY9s",
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.Linear1 = nn.Linear(num_inputs, num_outputs)\n",
    "        torch.nn.init.normal_(self.Linear1.weight, std=0.01) #init the weights\n",
    "        torch.nn.init.zeros_(self.Linear1.bias) #init the bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_inputs)\n",
    "        out = self.Linear1(x)\n",
    "        return out\n",
    "\n",
    "num_inputs, num_outputs = 784, 10\n",
    "net = Net(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7AFXUxIGY9s"
   },
   "source": [
    "# Alternative Initialization\n",
    "* This is useful if you have multiple layers of the same type and you want them to be initialized in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skDqydzpGY9s"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear): # by checking the type we can init different layers in different ways\n",
    "        torch.nn.init.normal_(m.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_weights);\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaASzNs2GY9s"
   },
   "source": [
    "# Loss Function\n",
    "\n",
    "* Use PyTorch's implementation of Softmax-Cross Entropy loss to avoid numerical instabilities\n",
    "    * The input to loss function are the logits $\\mathbf{o}$ (and not softmax outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH9P5TtZGY9t",
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saECPrJvGY9t",
    "origin_pos": 13
   },
   "source": [
    "# Optimization Algorithm\n",
    "\n",
    "* Minibatch SGD with a learning rate of 0.1 as the optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8gtOgxQGY9t",
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEXWEY4TGY9u"
   },
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T_UaTaY5GY9u"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):  #y_hat is a matrix; 2nd dimension stores prediction scores for each class.\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) # Predicted class is the index of max score         \n",
    "    cmp = (y_hat.type(y.dtype) == y)  # because`==` is sensitive to data types\n",
    "    return float(torch.sum(cmp)) # Taking the sum yields the number of correct predictions.\n",
    "\n",
    "# Example: only 1 sample is correctly classified.\n",
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "accuracy(y_hat, y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3xwoUAAGY9u"
   },
   "outputs": [],
   "source": [
    "class Accumulator:  \n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n # [0, 0, ..., 0]\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQKQSQ8CGY9v"
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter): \n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    for _, (X, y) in enumerate(data_iter):\n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvedhfBzGY9v"
   },
   "source": [
    "* The accuracy of the model prior to training should be close to random guessing, i.e., 0.1 for 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNZDhShjGY9v"
   },
   "outputs": [],
   "source": [
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH4rR-kwGY9v",
    "origin_pos": 17
   },
   "source": [
    "# Training\n",
    "\n",
    "* The training loop for softmax regression looks strikingly familiar with that of linear regression \n",
    "* Here we refactor the implementation to make it reusable.\n",
    "    * First, we define a function to train for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZKrex2DGY9w"
   },
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, optimizer, batch_size=256, num_outputs=10):  \n",
    "    \"\"\"The training function for one epoch.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel())\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzUmljsNGY9w"
   },
   "source": [
    "# Training\n",
    "\n",
    "* The following class will be used to plot training and validation accuracy as well as loss evolution over training loop \n",
    "* Not important to understand how this code works, we just need to use it for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TUr83txGY9x"
   },
   "outputs": [],
   "source": [
    "class Animator:  #@save\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        mu.use_svg_display()\n",
    "        self.fig, self.axes = mu.plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: mu.set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0Ecq_zOGY9x"
   },
   "source": [
    "# Training\n",
    "\n",
    "* The following function trains the model (`net`) on a training set (`train_iter`) for `num_epochs`.\n",
    "* At the end of each epoch, the model is evaluated on a testing set (`test_iter`).\n",
    "* `Animator` for visualizing the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKyLift_GY9x"
   },
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer):  #@save\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "                        legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, optimizer)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5M4Z_8flGY9y",
    "origin_pos": 18,
    "scrolled": false,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_UBK9rxGY9y"
   },
   "source": [
    "# Optional Tasks\n",
    "\n",
    "* Task 1: The model defined above has 1 Linear (or Fully-Connected Layer). Expand your `Net` class to have one more linear layer as follows: the 1st Linear layer will output `num_hidden` outputs. Set `num_hidden=40`. The second linear layer will have `num_hidden` inputs and 10 outputs (i.e. equal to the number of classes). Train this model and check its accuracy. Is it better than in terms of accuracy than your original model? Can you justify your answer? \n",
    "* Task 2: We trained our original model using CE Loss. Can you modify the code to train it with MSE Loss? Hints:  \n",
    "    * You will need to write some code to convert the labels into 1-hot encoding (do that inside the training loop).\n",
    "    * You might have to adjust the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "Week04_Lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
