{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if use Colab\n",
    "'''from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True) #mount google drive\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4d2kQ8WqNRwh",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Nothing better to begin with than by importing some necessary dependencies. \n",
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iTuBhPKNRwm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECS269U/P - Coursework\n",
    "\n",
    "\n",
    "* The **goal** of the CW is similar to that of Week 2's Lab: fitting a curve to data, also known as **curve fitting**. \n",
    "* This has applications in many different disciplines that make use of AI: FinTech, Physics Modelling, or even Sports. \n",
    "* For example, we might be interested in learning the evolution (over time) of the price of a specific product in different countries. This can depend on several factors: the product itself, the country, the initial value of the product's price, etc. \n",
    "* As usual, we are interested in learning a model that finds these relationships *from the data*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZukjW50JNRwn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning a family of functions\n",
    "\n",
    "* The main difference with Week 2's Lab is that we will learn a network that does not learn a single function but a *family of functions*.\n",
    "* We will consider a family of sinusoidal functions. \n",
    "* Below you can find the code generating the data according to different random sinusoidal functions $\\{f_a\\}$. We randomly generate a set of 40 points in the x-axis in the interval $[-2, 2]$, slightly randomly shifted. Our functions will have the form of $y = f_a(x) = a * sin(x+a)$ where each $a$ will be randomly sampled for each function from interval $[-2, 2]$.  To \"draw\" a function $f_a$, we first choose some $a \\sim U(-2,2)$ and then we compute $f_a$ using the above formula for all the $x$ in the x-axis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3lxK0rwYNRwn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Nf = 2000 # the number of different functions f that we will generate\n",
    "Npts = 40 # the number of x values that we will use to generate each fa\n",
    "x = torch.zeros(Nf, Npts, 1)\n",
    "for k in range(Nf):\n",
    "    x[k,:,0] = torch.linspace(-2, 2, Npts)\n",
    "\n",
    "x += torch.rand_like(x)*0.1\n",
    "a = -2 + 4*torch.rand(Nf).view(-1,1).repeat(1, Npts).unsqueeze(2)\n",
    "y = a*torch.sin(x+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq0EeX9cNRwo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Learning Goal\n",
    "\n",
    "* Because we are dealing with a family of functions and not just a single function, our model must be able to perform two tasks: *Function Selection* and *Regression*.\n",
    "* Function selection means that given some *additional* input (to be defined below) the model somehow must choose which function $f_a$ from the family of functions $\\{f_a\\}$ it needs to model.\n",
    "* Once the correct function is picked then the model must perform regression i.e. learn the relationship $y=f_a(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkJ9LSYWNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## The Learning Objective\n",
    "\n",
    "* During training we randomly sample functions from the family of functions $\\{f_a\\}$. For each $f_a$, we are provided with the (input, output) pairs $(x_t, y_t), t=1,\\dots,N_{pts}$.\n",
    "\n",
    "* To perform *Function Selection*, a **random subset** of $(x_t, y_t), t=1,\\dots,N_{pts}$ is provided as auxiliary input to the model during *both training and testing*. These auxiliary data is called the *context data:* $(x_c, y_c), c=1,\\dots,N_c$. \n",
    "\n",
    "* Note that the total number of context points $N_c$ should be different (and randomly chosen) for every batch so that the model learns to handle different number of context points at test time. This means that the model should be able to work for e.g. $N_c=5$ but also for $N_c=12$ etc.\n",
    "\n",
    "* Our model will take the context pairs $(x_c, y_c)$ and input values $x_t$ and will produce the estimated values $\\hat{y}_t$. \n",
    "\n",
    "* During training we have access to the ground-truth values $y_t$, and thus we can compute a loss between the model's predictions $\\hat{y}_t$ and the ground-truth values $y_t$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAr6FPwWNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Model\n",
    "\n",
    "* Our model will consists of 2 MLPs which must be jointly trained.\n",
    "* The first MLP is called the *Context Encoder* or Encoder. The Encoder will take as input each pair $(x_c, y_c)$ and will produce a corresponding feature representation $r_c$ of dimension $r_{dim}$.\n",
    "* A total context feature is produced by averaging over all features: $r_C= \\frac{1}{N}\\sum_c r_c$.\n",
    "* The second MLP is called the Decoder. It takes as input the $r_C$ and each input data $x_t$ and produces the model's prediction $\\hat{y}_t$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKgPk6MbNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Encoder-Decoder](CW1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dni9bGa2NRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "* The Encoder and the Decoder are **MLPs**. You can experiment with your own architectures. You can also choose to implement the following architectures:\n",
    "    * *Encoder*: It will map the input pair $(x_c, y_c)$ to some features of dimension $h_{dim}$ using 2 *hidden* layers. A final layer will produce the feature representation $r_c$ of dimension $r_{dim}$.\n",
    "    * *Decoder*: It will map the input pair $(r_C, x_t)$ to some features of dimension $h_{dim}$ using 2 *hidden* layers. A final layer will produce the model's prediction $\\hat{y}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1COz3usNRwq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tasks\n",
    "\n",
    "* You have to implement the following:\n",
    "    1. Create the training dataset and dataloader (10%). \n",
    "    2. Create the Encoder and Decoder (20 + 20%). \n",
    "    3. Create the optimizer and the loss for your model (10%).\n",
    "    4. Write the training script that will train the model and print the training loss (30%).\n",
    "    5. Evaluate the model on some validation data. Plot some predictions. (10%). \n",
    "\n",
    "* You might want to explore the impact of the following design choices and hyperparameters:\n",
    "    1. Number of hidden layers and $h_{dim}$, and  $r_{dim}$.\n",
    "    1. Type of optimizer, batch-size and all relevant hyper-parameters from Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data  \n",
    "\n",
    "* Test data are stored in a dictionary where each key has the data for a single function $f_a$. We have generated 6 different functions named as `function_num_1`, `function_num_2` and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_to_the_pickle = '/content/gdrive/MyDrive/Colab Notebooks/test_data.pkl'\n",
    "test_data =pickle.load(open(path_to_the_pickle,'rb'))\n",
    "# 6 test functions\n",
    "i = 2 # i=1,..6\n",
    "name= 'function_num_{}'.format(i)\n",
    "x_c=test_data[name]['context_pairs'][0]\n",
    "y_c=test_data[name]['context_pairs'][1]\n",
    "x_t =test_data[name]['x']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW_v3_with_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
