{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncomment if use Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True) #mount google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d2kQ8WqNRwh",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "import random\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iTuBhPKNRwm",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ECS269U/P - Coursework\n",
    "\n",
    "\n",
    "* The **goal** of the CW is similar to that of Week 2's Lab: fitting a curve to data, also known as **curve fitting**. \n",
    "* This has applications in many different disciplines that make use of AI: FinTech, Physics Modelling, or even Sports. \n",
    "* For example, we might be interested in learning the evolution (over time) of the price of a specific product in different countries. This can depend on several factors: the product itself, the country, the initial value of the product's price, etc. \n",
    "* As usual, we are interested in learning a model that finds these relationships *from the data*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZukjW50JNRwn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning a family of functions\n",
    "\n",
    "* The main difference with Week 2's Lab is that we will learn a network that does not learn a single function but a *family of functions*.\n",
    "* We will consider a family of sinusoidal functions. \n",
    "* Below you can find the code generating the data according to different random sinusoidal functions $\\{f_a\\}$. We randomly generate a set of 40 points in the x-axis in the interval $[-2, 2]$, slightly randomly shifted. Our functions will have the form of $y = f_a(x) = a * sin(x+a)$ where each $a$ will be randomly sampled for each function from interval $[-2, 2]$.  To \"draw\" a function $f_a$, we first choose some $a \\sim U(-2,2)$ and then we compute $f_a$ using the above formula for all the $x$ in the x-axis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lxK0rwYNRwn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Nf = 2000 # the number of different functions f that we will generate\n",
    "Npts = 40 # the number of x values that we will use to generate each fa\n",
    "x = torch.zeros(Nf, Npts, 1)\n",
    "for k in range(Nf):\n",
    "    x[k,:,0] = torch.linspace(-2, 2, Npts)\n",
    "\n",
    "x += torch.rand_like(x)*0.1\n",
    "a = -2 + 4*torch.rand(Nf).view(-1,1).repeat(1, Npts).unsqueeze(2)\n",
    "y = a*torch.sin(x+a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq0EeX9cNRwo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Learning Goal\n",
    "\n",
    "* Because we are dealing with a family of functions and not just a single function, our model must be able to perform two tasks: *Function Selection* and *Regression*.\n",
    "* Function selection means that given some *additional* input (to be defined below) the model somehow must choose which function $f_a$ from the family of functions $\\{f_a\\}$ it needs to model.\n",
    "* Once the correct function is picked then the model must perform regression i.e. learn the relationship $y=f_a(x)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkJ9LSYWNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## The Learning Objective\n",
    "\n",
    "* During training we randomly sample functions from the family of functions $\\{f_a\\}$. For each $f_a$, we are provided with the (input, output) pairs $(x_t, y_t), t=1,\\dots,N_{pts}$.\n",
    "\n",
    "* To perform *Function Selection*, a **random subset** of $(x_t, y_t), t=1,\\dots,N_{pts}$ is provided as auxiliary input to the model during *both training and testing*. These auxiliary data is called the *context data:* $(x_c, y_c), c=1,\\dots,N_c$. \n",
    "\n",
    "* Note that the total number of context points $N_c$ should be different (and randomly chosen) for every batch so that the model learns to handle different number of context points at test time. This means that the model should be able to work for e.g. $N_c=5$ but also for $N_c=12$ etc.\n",
    "\n",
    "* Our model will take the context pairs $(x_c, y_c)$ and input values $x_t$ and will produce the estimated values $\\hat{y}_t$. \n",
    "\n",
    "* During training we have access to the ground-truth values $y_t$, and thus we can compute a loss between the model's predictions $\\hat{y}_t$ and the ground-truth values $y_t$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAr6FPwWNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Model\n",
    "\n",
    "* Our model will consists of 2 MLPs which must be jointly trained.\n",
    "* The first MLP is called the *Context Encoder* or Encoder. The Encoder will take as input each pair $(x_c, y_c)$ and will produce a corresponding feature representation $r_c$ of dimension $r_{dim}$.\n",
    "* A total context feature is produced by averaging over all features: $r_C= \\frac{1}{N}\\sum_c r_c$.\n",
    "* The second MLP is called the Decoder. It takes as input the $r_C$ and each input data $x_t$ and produces the model's prediction $\\hat{y}_t$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKgPk6MbNRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Encoder-Decoder](CW1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dni9bGa2NRwp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "* The Encoder and the Decoder are **MLPs**. You can experiment with your own architectures. You can also choose to implement the following architectures:\n",
    "    * *Encoder*: It will map the input pair $(x_c, y_c)$ to some features of dimension $h_{dim}$ using 2 *hidden* layers. A final layer will produce the feature representation $r_c$ of dimension $r_{dim}$.\n",
    "    * *Decoder*: It will map the input pair $(r_C, x_t)$ to some features of dimension $h_{dim}$ using 2 *hidden* layers. A final layer will produce the model's prediction $\\hat{y}_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1COz3usNRwq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tasks\n",
    "\n",
    "* You have to implement the following:\n",
    "    1. Create the training dataset and dataloader (10%). \n",
    "    2. Create the Encoder and Decoder (20 + 20%). \n",
    "    3. Create the optimizer and the loss for your model (10%).\n",
    "    4. Write the training script that will train the model and print the training loss (30%).\n",
    "    5. Evaluate the model on some validation data. Plot some predictions. (10%). \n",
    "\n",
    "* You might want to explore the impact of the following design choices and hyperparameters:\n",
    "    1. Number of hidden layers and $h_{dim}$, and  $r_{dim}$.\n",
    "    1. Type of optimizer, batch-size and all relevant hyper-parameters from Week 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data  \n",
    "\n",
    "* Test data are stored in a dictionary where each key has the data for a single function $f_a$. We have generated 6 different functions named as `function_num_1`, `function_num_2` and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 Create the training dataset and dataloader (10%)\n",
    "\n",
    "# Create dataset using the randomly generated x,y values\n",
    "dataset = data.TensorDataset(x, y)\n",
    "# Create Dataloader, shuffle is set to true so that the data is reshuffled at every epoch \n",
    "data_iter =  data.DataLoader(dataset, batch_size=40, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 Create the Encoder and Decoder (20 + 20%)\n",
    "\n",
    "# define Encoder MLP\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_inputs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.Linear1 = torch.nn.Linear(1,1600)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.Linear2 = torch.nn.Linear(1,num_inputs*n_c,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         hidden layer for x values\n",
    "        x_layer = self.Linear1(x[0]) \n",
    "        x_layer = self.ReLU(x_layer)\n",
    "#         hidden layer for y values\n",
    "        y_layer = self.Linear2(x[1])\n",
    "        y_layer = self.ReLU(y)\n",
    "#         define empty list of x and y values\n",
    "        xr_c = []\n",
    "        yr_c = []\n",
    "#         define counter so that each x and y value can be iterated through\n",
    "        encounter = 0\n",
    "        for value in x[0]:\n",
    "#             average each set of x and y values\n",
    "            averagex = torch.mean(x_layer[encounter])\n",
    "            averagey = torch.mean(y_layer[encounter])\n",
    "#             add the average of each set of x and y values to the list defined \n",
    "            xr_c.append(averagex)\n",
    "            yr_c.append(averagey)\n",
    "#             add 1 to the iterator as the for loop has come to an end\n",
    "            encounter += 1\n",
    "#         find the average of all the x and y values to create the components of the total context feature \n",
    "        avgxr_c = sum(xr_c)/len(xr_c)\n",
    "        avgyr_c = sum(yr_c)/len(yr_c)\n",
    "#         place the x and y averages into a tuple to create r_c, the total context feature\n",
    "        r_c = [avgxr_c, avgyr_c]\n",
    "        return r_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Decoder MLP\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, max_n_c):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.n_c = n_c\n",
    "        self.Linear1 = torch.nn.Linear(3,1)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.Linear2 = torch.nn.Linear(1,num_inputs*max_n_c,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x is a tuple of r_c and x_t, split these out into variables to make code clearer \n",
    "        r_c = x[0]\n",
    "        x_t = x[1][0]\n",
    "#         define iterator\n",
    "        i = 0\n",
    "        for layer in x[1]:\n",
    "            for x_t_value in x[1][0]:\n",
    "#                  create tensor of r_c and x_t\n",
    "                r_c_x_t = torch.tensor([r_c[0], r_c[1], x_t_value])\n",
    "#                  put r_c_x_t values into hidden layers \n",
    "                y_hat = self.Linear1(r_c_x_t)\n",
    "                y_hat = self.ReLU(y_hat)\n",
    "                y_hat = self.Linear2(y_hat)\n",
    "                y_hat = y_hat.view(-1, self.num_inputs, 1)\n",
    "            i += 1\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalise Models \n",
    "max_n_c = 40\n",
    "net = Encoder(Npts)\n",
    "decoder = Decoder(Npts, max_n_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot training loss \n",
    "def plot_training_loss(training_loss):\n",
    "    # define list in range 0 to the number of epochs, in our case 0 to 50\n",
    "    epoch_list = list(range(0, epochs))\n",
    "    # plot loss at each epoch\n",
    "    plt.plot(epoch_list,training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 Write the training script that will train the model tand print the training loss (30%)\n",
    "\n",
    "# training method\n",
    "def train(net, decoder, train_iter, loss, optimizer, epochs):\n",
    "#     define empty list for training_loss\n",
    "    training_loss = []\n",
    "#     loop on every epoch\n",
    "    for epoch in range(epochs):\n",
    "#         loop on every value in X and y\n",
    "        for X, y in train_iter:\n",
    "#             define number of context points - random number between 3 and 35\n",
    "            n_c = random.randint(3, 35)\n",
    "            x_tensor = X\n",
    "#             only take first n_c values of x to get model used to handling different number of context points\n",
    "            x_split_tensor = torch.split(x_tensor, n_c)[0]\n",
    "            y_tensor = y\n",
    "            y_split_tensor = torch.split(y_tensor, n_c)[0]\n",
    "            optimizer.zero_grad()\n",
    "#             encoder returns total context feature r_c\n",
    "            r_c = net([x_split_tensor[0], x_split_tensor[0]])\n",
    "#             decoder returns y_hat, a prediction\n",
    "            y_hat = decoder([r_c, x_tensor])\n",
    "#             find loss of model, comparing models prediction against raw y value\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "#         add training loss at the end of each epoch to list of training loss list\n",
    "        training_loss.append(l.detach().numpy())\n",
    "#     call function to plot training loss \n",
    "    plot_training_loss(training_loss)\n",
    "#     traing method returns its prediction for y\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 Create the optimizer and the loss for your model (10%)\n",
    "\n",
    "# define loss as MSELoss\n",
    "loss = torch.nn.MSELoss()\n",
    "# define learning rate\n",
    "lr = 0.01\n",
    "# define number of epochs\n",
    "epochs = 10\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr)\n",
    "# call training method\n",
    "y_hat = train(net, decoder, data_iter, loss, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate the model on some validation data. Plot some predictions (10%)\n",
    "\n",
    "# define test method\n",
    "def test(net, decoder, test_iter, loss, optimizer, x_t):\n",
    "#      loop on every value in X and y\n",
    "    for X, y in test_iter:\n",
    "        optimizer.zero_grad()\n",
    "#         encoder returns total context feature r_c\n",
    "        r_c = net([X, y])\n",
    "#         decoder returns y_hat prediction\n",
    "        y_hat = decoder([r_c, x_t])\n",
    "        optimizer.step()\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_the_pickle = '/content/gdrive/My Drive/Colab Notebooks/test_data.pkl'\n",
    "test_data =pickle.load(open(path_to_the_pickle,'rb'))\n",
    "# 6 test functions loop through each one\n",
    "for i in range(1,7):\n",
    "    name= 'function_num_{}'.format(i)\n",
    "    x_c=test_data[name]['context_pairs'][0]\n",
    "    y_c=test_data[name]['context_pairs'][1]\n",
    "    x_t =test_data[name]['x']\n",
    "#     create dataset of the test data\n",
    "    test_dataset = data.TensorDataset(x_c, y_c)\n",
    "#     create dataloader of dataloader\n",
    "    test_data_iter =  data.DataLoader(dataset, n_c)\n",
    "#     get models y prediction\n",
    "    y_hat = test(net, decoder, test_data_iter, loss, optimizer, x_t)\n",
    "#     create new plot canvas\n",
    "    plt.figure(i)\n",
    "#     plot models y prediction against the context data\n",
    "    plt.plot(x_c[0,:,0].to('cpu'), y_c[0,:,0].to('cpu'), '*')\n",
    "    plt.plot(x_t[0,:,0].to('cpu'), y_hat[0,:,0].detach().to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CW_v3_with_answers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
